# Databricks notebook source
# MAGIC %md
# MAGIC # Deep Clone a Schema
# MAGIC
# MAGIC Databricks' Deep Clone functionality enables the effortless creation of a data replica with minimal coding and maintenance overhead. Using the `CLONE` command, you can efficiently generate a duplicate of an existing Delta Lake table on Databricks at a designated version. The cloning process is incremental, ensuring that only new changes since the last clone are applied to the table. 
# MAGIC
# MAGIC
# MAGIC Deep cloning is applied on a per-table basis, requiring a separate invocation for each table within your schema. In scenarios where automation is desirable, such as when dealing with shared schemas through Delta sharing, replicating the entire schema can be achieved using DiscoverX. This approach eliminates the need to manually inspect and modify your code each time a new table is added to the schema by the provider.
# MAGIC
# MAGIC This notebook serves as an example of utilizing DiscoverX to automate the replication of a schema using Delta Deep Clone.

# COMMAND ----------

# MAGIC %md
# MAGIC ## [Optional] Create source and destination for the example

# COMMAND ----------

# MAGIC %sql
# MAGIC create catalog if not exists _discoverx_deep_clone;
# MAGIC create schema if not exists _discoverx_deep_clone.samples;
# MAGIC use catalog _discoverx_deep_clone;
# MAGIC use schema samples;
# MAGIC
# MAGIC CREATE OR REPLACE TABLE num_table
# MAGIC   (id BIGINT GENERATED BY DEFAULT AS IDENTITY, num INT);
# MAGIC
# MAGIC INSERT INTO num_table(num)
# MAGIC SELECT explode(sequence(1, 200, 2));
# MAGIC
# MAGIC CREATE OR REPLACE TABLE date_table (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   date_col DATE
# MAGIC );
# MAGIC
# MAGIC INSERT INTO date_table(date_col)
# MAGIC SELECT explode(sequence(CAST(DATE'2022-01-01' AS TIMESTAMP), CAST(DATE'2023-12-01' AS TIMESTAMP), INTERVAL 1 MONTH));
# MAGIC
# MAGIC CREATE OR REPLACE VIEW num_view AS SELECT * FROM num_table;

# COMMAND ----------

# MAGIC %md
# MAGIC ### Specify a source and a destination catalog

# COMMAND ----------

dbutils.widgets.text("1.source_catalog", "_discoverx_deep_clone") 
dbutils.widgets.text("2.destination_catalog", "_discoverx_deep_clone_replica")

source_catalog = dbutils.widgets.get("1.source_catalog")
destination_catalog = dbutils.widgets.get("2.destination_catalog")

# COMMAND ----------

print(f"Replicate data from {source_catalog} to {destination_catalog} ")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Use DiscoverX for cloning

# COMMAND ----------

# %pip install dbl-discoverx==0.0.7
# dbutils.library.restartPython()

# COMMAND ----------

from discoverx import DX

dx = DX()

# COMMAND ----------

spark.sql(f"CREATE CATALOG IF NOT EXISTS {destination_catalog}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Define a function for cloning

# COMMAND ----------

def clone_tables(table_info):
  
  spark.sql(f"CREATE SCHEMA IF NOT EXISTS {destination_catalog}.{table_info.schema}")
  
  try:
    spark.sql(
    f"""CREATE OR REPLACE TABLE 
    {destination_catalog}.{table_info.schema}.{table_info.table} 
    CLONE {table_info.catalog}.{table_info.schema}.{table_info.table}
    """
  )
    info = f"Cloned {table_info.catalog}.{table_info.schema}.{table_info.table} to {destination_catalog}.{table_info.schema}.{table_info.table}" 
  # Cloning Views is not supported
  except Exception as error: 
    info = f"{error}  Cloning Skipped "
  
  return info

# COMMAND ----------

# MAGIC %md
# MAGIC ## Apply the custom fuctions for multiple tables

# COMMAND ----------

dx.from_tables(f"{source_catalog}.*.*")\
  .map(clone_tables)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Clean up

# COMMAND ----------

spark.sql(f"DROP CATALOG IF EXISTS {destination_catalog} cascade")

# COMMAND ----------


